{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215e1243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kms10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169da75e",
   "metadata": {},
   "source": [
    "# 1. Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaeeb65",
   "metadata": {},
   "source": [
    "## (1) 문장 토큰화 (Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf3ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"\n",
    "Please access the server with IP address 192.168.56.31, save the log file, \n",
    "and send it to aaa@gmail.com. After that, let's go eat lunch. \n",
    "Think about what you want to eat.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08ae09",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>그냥 마침표를 기준으로 split하면 결과가 아래와 같습니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be11a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = my_str.split('.')\n",
    "result = []\n",
    "for token in sent_tokens:\n",
    "    sent_token = token.replace('\\n','')\n",
    "    result.append(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2137a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please access the server with IP address 192',\n",
       " '168',\n",
       " '56',\n",
       " '31, save the log file, and send it to aaa@gmail',\n",
       " 'com',\n",
       " \" After that, let's go eat lunch\",\n",
       " ' Think about what you want to eat',\n",
       " '']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac77c8c",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>nltk의 sent_tokenize()를 이용하세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e5cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = my_str.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c06205f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\kms10/nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sent_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\kms10/nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\kms10\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460abcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a29ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for token in sent_tokens:\n",
    "    sent_token = token.replace('\\n','') # 줄바꿈 문자 제거 (\\n). Remove new line character(\\n)\n",
    "    result.append(sent_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8488fd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please access the server with IP address 192.168.56.31, save the log file, and send it to aaa@gmail.com.',\n",
       " \"After that, let's go eat lunch.\",\n",
       " 'Think about what you want to eat.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016534eb",
   "metadata": {},
   "source": [
    "## (2) 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52b33140",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69526a53",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>특수문자를 제거한 뒤, 공백문자를 기준으로 split하면 결과가 다음과 같습니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7100363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dont be fooled by the dark sounding name MrJones Orphanage is as cheery as cheery goes for a pastry shop\n"
     ]
    }
   ],
   "source": [
    "result = my_str.replace(\"'\", \"\").replace(\".\",\"\").replace(\",\",\"\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d351b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dont', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'MrJones', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "result = result.split(' ')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cbbabf",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>nltk.word_tokenize()</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87edd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "result = nltk.word_tokenize(my_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe92f90",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>nltk.wordpunct_tokenize()</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56112421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "result = nltk.wordpunct_tokenize(my_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01aa7f",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>nltk.TreebankWordTokenizer()</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>표준 토큰화 방법인 Penn Treebank Tokenization 규칙에 기반한 함수</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>Rule 1. 하이픈(-)으로 구성된 단어는 하나로 유지</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>Rule 2. doesn't와 같이 아포스트로피(')로 '접어'가 함께하는 단어는 분리</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4067f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6afee7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "result = tokenizer.tokenize(my_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aaa7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', '.', 'It', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n",
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'It', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "my_str = \"Starting a home-based restaurant may be an ideal. It doesn't have a food chain or restaurant of their own.\"\n",
    "print(nltk.word_tokenize(my_str))\n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "result = tokenizer.tokenize(my_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba496e9",
   "metadata": {},
   "source": [
    "## (3) POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2700870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students.', 'And', 'you', 'are', 'a', 'Ph.D.', 'student.', 'Would', 'you', 'like', 'to', 'join', 'my', 'laboratory', '?']\n"
     ]
    }
   ],
   "source": [
    "my_str = \"I am actively looking for Ph.D. students. And you are a Ph.D. student. Would you like to join my laboratory?\"\n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "result = tokenizer.tokenize(my_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f417cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students.', 'NN'), ('And', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student.', 'NN'), ('Would', 'NNP'), ('you', 'PRP'), ('like', 'IN'), ('to', 'TO'), ('join', 'VB'), ('my', 'PRP$'), ('laboratory', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.tag.pos_tag(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c78b0",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.1em;line-height:1.5em'>PRP: 인칭대명사(personal pronoun)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>VBP: 동사(verb, sing. present, non-3d)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>RB: 부사(adverb)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>VBG: 현재분사(verb, gerund/present participle)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>IN: 전치사(preposition, subordiateing conjunction)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>NNP: 고유 명사(proper noun, singular)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>NNS: 복수 명사(noun, plural)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>CC: 접속사(coordinating conjunction)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>DT: 관사(determiner)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>NN: 단수 명사(noun, singular)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>TO: to</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>VB: 동사 원형(verb, base form)</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>PRP$: 소유대명사(possessive pronoun)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a65d6",
   "metadata": {},
   "source": [
    "## (4) RegexpTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f293",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>NLTK의 정규 표현식을 사용해서 단어 토큰화를 수행하는 RegexpTokenizer()</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9666c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"\n",
    "Don't be fooled by the dark sounding name, \n",
    "Mr.Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6470a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = nltk.tokenize.RegexpTokenizer('[a-zA-Z0-9]+')\n",
    "tokenizer2 = nltk.tokenize.RegexpTokenizer('\\s+', gaps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96afa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'Mr', 'Jone', 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n",
      "[\"Don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name,', \"Mr.Jone's\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1.tokenize(my_str))\n",
    "print(tokenizer2.tokenize(my_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fbb7d",
   "metadata": {},
   "source": [
    "## 2. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005d8ba",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.1em;line-height:1.5em'>Text Cleaning에 다음 step만 들어간다는게 아닙니다.</span>\n",
    "- <span style = 'font-size:1.1em;line-height:1.5em'>단지, 예시일 뿐 사용하는 말뭉치에 따라 여러가지 cleaning 작업을 해야 합니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0889ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = '''\n",
    "\"Hey Amazon - my package never arrived \n",
    "https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first \n",
    "PLEASE FIX ASAP! @AmazonHelp\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db242d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"Hey Amazon - my package never arrived \\nhttps://www.amazon.com/gp/css/order-history?ref_=nav_orders_first \\nPLEASE FIX ASAP! @AmazonHelp\"'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4126f6",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Case Normalization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dea1ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\"\n"
     ]
    }
   ],
   "source": [
    "my_str = my_str.replace('\\n','')\n",
    "result = my_str.lower()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762f311",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>다음과 같은 추가 작업을 해야합니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>URL 삭제</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>Unicode characters 삭제\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>해시태그(@amazonhelp) 삭제</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f97dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f693022",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tmp = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57ec6867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"hey amazon - my package never arrived https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first please fix asap! @amazonhelp\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f0ea377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hey amazon   my package never arrived   please fix asap    \n"
     ]
    }
   ],
   "source": [
    "p = re.compile(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\")\n",
    "result = p.sub(' ', result_tmp)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5256f8",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Stopword removal</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "805caf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b1a3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4649a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61950e9",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Add more stopwords</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69e36b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.extend(['hey', 'amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3da28de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'hey', 'amazon']\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42dd78a",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>After stopword removal</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a81fb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hey amazon   my package never arrived   please fix asap    '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54b0dc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'amazon', 'my', 'package', 'never', 'arrived', 'please', 'fix', 'asap']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4ac118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['package', 'never', 'arrived', 'please', 'fix', 'asap']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in result.split() if word not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73bab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ' '.join([word for word in result.split() if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0adecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'package never arrived please fix asap'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445eb3eb",
   "metadata": {},
   "source": [
    "# 3. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b672b445",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>nltk의 WordNetLemmatizer를 이용하세요.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65f70974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "550b7d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"I am actively looking for Ph.D. students. \n",
    "And you are a Ph.D. student. \n",
    "Would you like to join my laboratory?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09c2decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am actively looking for Ph.D. students. And you are a Ph.D. student. Would you like to join my laboratory?\n"
     ]
    }
   ],
   "source": [
    "my_str = my_str.replace('\\n','') # 줄바꿈문자(\\n) 제거. Remove next line character(\\n)\n",
    "print(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fcefcfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for Ph.D. students.', 'And you are a Ph.D. student.', 'Would you like to join my laboratory?']\n"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(my_str)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd68ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특수문자 제거전 :  ['i', 'am', 'actively', 'looking', 'for', 'ph.d.', 'students', '.']\n",
      "특수문자 제거후 :  ['i', 'am', 'actively', 'looking', 'for', 'ph.d.', 'students']\n",
      "특수문자 제거전 :  ['and', 'you', 'are', 'a', 'ph.d.', 'student', '.']\n",
      "특수문자 제거후 :  ['and', 'you', 'are', 'a', 'ph.d.', 'student']\n",
      "특수문자 제거전 :  ['would', 'you', 'like', 'to', 'join', 'my', 'laboratory', '?']\n",
      "특수문자 제거후 :  ['would', 'you', 'like', 'to', 'join', 'my', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "my_str = my_str.replace('\\n','') # 줄바꿈문자(\\n) 제거 (Remove new line char)\n",
    "sent_tokens = nltk.sent_tokenize(my_str)\n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "p = re.compile(r\"^[^0-9A-Za-z]+\") # 특수문자 제거 (Remove special char)\n",
    "\n",
    "results = []\n",
    "for sent in sent_tokens:\n",
    "    sent = sent.lower() # 문장을 소문자로 변환 (Convert the sentence to lower case)\n",
    "    result = tokenizer.tokenize(sent) # 각 문장을 단어 토큰화 (Tokenize sentence into words)\n",
    "    print('특수문자 제거전 : ', result)\n",
    "    for token in result: # 각 단어 토큰에 대해서 (For, each word token)\n",
    "        if p.match(token): # 특수문자이면 (If it is special charcater)\n",
    "            result.remove(token) # 문장의 단어 토큰에서 제거한다 (Remove the word token)\n",
    "    print('특수문자 제거후 : ', result)\n",
    "    results.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "58a48130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'am',\n",
       " 'actively',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'ph.d.',\n",
       " 'students',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'a',\n",
       " 'ph.d.',\n",
       " 'student',\n",
       " 'would',\n",
       " 'you',\n",
       " 'like',\n",
       " 'to',\n",
       " 'join',\n",
       " 'my',\n",
       " 'laboratory']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1afc0184",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = nltk.tag.pos_tag(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f35bc5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('ph.d.', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('ph.d.', 'JJ'),\n",
       " ('student', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('join', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('laboratory', 'NN')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87e69ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Lemmatization: \n",
      "['i', 'am', 'actively', 'looking', 'for', 'ph.d.', 'students', 'and', 'you', 'are', 'a', 'ph.d.', 'student', 'would', 'you', 'like', 'to', 'join', 'my', 'laboratory']\n",
      "After Lemmatization: \n",
      "['i', 'am', 'actively', 'looking', 'for', 'ph.d.', 'student', 'and', 'you', 'are', 'a', 'ph.d.', 'student', 'would', 'you', 'like', 'to', 'join', 'my', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Before Lemmatization: \")\n",
    "print(results)\n",
    "print(\"After Lemmatization: \")\n",
    "print([lemmatizer.lemmatize(word) for word in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ad8d5",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Penn Treebank에 기반한 POS Tag를 WordNet Lemmatizer에 그대로 사용하면 안됩니다.</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>Penn Treebank에서 사용하는 POS Tag set과 WordNet에서 사용하는 Tag set이 다르기 때문입니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ed1a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8259357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('ph.d.', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('ph.d.', 'JJ'),\n",
       " ('student', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('you', 'PRP'),\n",
       " ('like', 'IN'),\n",
       " ('to', 'TO'),\n",
       " ('join', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('laboratory', 'NN')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96353181",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_tags = [(tag[0], convert_tag(tag[1])) for tag in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db937789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'n'),\n",
       " ('am', 'v'),\n",
       " ('actively', 'r'),\n",
       " ('looking', 'v'),\n",
       " ('for', None),\n",
       " ('ph.d.', 'a'),\n",
       " ('students', 'n'),\n",
       " ('and', None),\n",
       " ('you', None),\n",
       " ('are', 'v'),\n",
       " ('a', None),\n",
       " ('ph.d.', 'a'),\n",
       " ('student', 'n'),\n",
       " ('would', None),\n",
       " ('you', None),\n",
       " ('like', None),\n",
       " ('to', None),\n",
       " ('join', 'v'),\n",
       " ('my', None),\n",
       " ('laboratory', 'n')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98093238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전: \n",
      "['i', 'am', 'actively', 'looking', 'for', 'ph.d.', 'students', 'and', 'you', 'are', 'a', 'ph.d.', 'student', 'would', 'you', 'like', 'to', 'join', 'my', 'laboratory']\n",
      "표제어 추출 후: \n",
      "['i', 'be', 'actively', 'look', 'for', 'ph.d.', 'student', 'and', 'you', 'be', 'a', 'ph.d.', 'student', 'would', 'you', 'like', 'to', 'join', 'my', 'laboratory']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"표제어 추출 전: \")\n",
    "print(results)\n",
    "\n",
    "converted_tags = [(tag[0], convert_tag(tag[1])) for tag in tags]\n",
    "lemmas = []\n",
    "for tag in converted_tags:\n",
    "    if tag[1] is None:\n",
    "        lemmas.append(lemmatizer.lemmatize(tag[0]))\n",
    "    else:\n",
    "        lemmas.append(lemmatizer.lemmatize(tag[0], tag[1]))\n",
    "        \n",
    "print(\"표제어 추출 후: \")\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29a64f",
   "metadata": {},
   "source": [
    "# 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "456d14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = \"\"\"\n",
    "This was not the map we found in Billy Bones's chest, \n",
    "but an accurate copy, complete in all things--names and heights and soundings--\n",
    "with the single exception of the red crosses and the written notes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ef8a6",
   "metadata": {},
   "source": [
    "## (1) PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b123da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3721201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = my_str.replace('\\n','') # 줄바꿈문자(\\n) 제거 (Remove new line char)\n",
    "sent_tokens = nltk.sent_tokenize(my_str)\n",
    "tokenizer = nltk.TreebankWordTokenizer()\n",
    "p = re.compile(r\"^[^0-9A-Za-z]+\") # 특수문자 제거 (Remove special char)\n",
    "\n",
    "results = []\n",
    "for sent in sent_tokens:\n",
    "    sent = sent.lower() # 문장을 소문자로 변환 (Convert each sentence to lower case)\n",
    "    result = tokenizer.tokenize(sent) # 각 문장을 단어 토큰화 (Tokenize each sentence to word tokens)\n",
    "    for token in result: # 각 단어 토큰에 대해서 (For each word token,)\n",
    "        if p.match(token): # 특수문자이면 (if it is a special char)\n",
    "            result.remove(token) # 문장의 단어 토큰에서 제거한다 (Remove the token)\n",
    "    results.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7f77fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming\n",
      "['this', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'billy', 'bones', 'chest', 'but', 'an', 'accurate', 'copy', 'complete', 'in', 'all', 'things', 'names', 'and', 'heights', 'and', 'soundings', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes']\n",
      "\n",
      "After stemming\n",
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', 'chest', 'but', 'an', 'accur', 'copi', 'complet', 'in', 'all', 'thing', 'name', 'and', 'height', 'and', 'sound', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before stemming\")\n",
    "print(results)\n",
    "print()\n",
    "print(\"After stemming\")\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(word) for word in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90b558",
   "metadata": {},
   "source": [
    "## (2) Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fc7d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "65b21ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stemming\n",
      "['this', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'billy', 'bones', 'chest', 'but', 'an', 'accurate', 'copy', 'complete', 'in', 'all', 'things', 'names', 'and', 'heights', 'and', 'soundings', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes']\n",
      "\n",
      "After stemming\n",
      "['thi', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'bil', 'bon', 'chest', 'but', 'an', 'acc', 'cop', 'complet', 'in', 'al', 'thing', 'nam', 'and', 'height', 'and', 'sound', 'with', 'the', 'singl', 'exceiv', 'of', 'the', 'red', 'cross', 'and', 'the', 'writ', 'not']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before stemming\")\n",
    "print(results)\n",
    "print()\n",
    "print(\"After stemming\")\n",
    "stemmer = LancasterStemmer()\n",
    "print([stemmer.stem(word) for word in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7d53b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
